Error traceback:

Traceback (most recent call last):
  File "/Users/marcuschiam/gemini_flight_finder/model.py", line 163, in generate_response
    response = Response(self.chats.send_message(prompt_str))
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/chats.py", line 254, in send_message
    response = self._modules.generate_content(
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/models.py", line 5218, in generate_content
    response = self._generate_content(
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/models.py", line 4000, in _generate_content
    response = self._api_client.request(
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/_api_client.py", line 1388, in request
    response = self._request(http_request, http_options, stream=False)
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/_api_client.py", line 1224, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/_api_client.py", line 1201, in _request_once
    errors.APIError.raise_for_response(response)
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/errors.py", line 106, in raise_for_response
    cls.raise_error(response.status_code, response_json, response)
  File "/Users/marcuschiam/env/fli/lib/python3.10/site-packages/google/genai/errors.py", line 133, in raise_error
    raise ServerError(status_code, response_json, response)
google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE', 'details': [{'@type': 'type.googleapis.com/google.rpc.DebugInfo', 'detail': '[ORIGINAL ERROR] generic::unavailable: [original: extensible_stubs::UNABLE_TO_RETRY] Fail to execute model for flow_id: gemini_v3p1s_rev19_calmriver\nError: A retriable error could not be retried because a stream payload was received from the server (see go/xs-unable-to-retry-message-received) (old status: generic::unavailable: Preempted out of decode queue by a higher priority request.;  Failed to run inference for model: go/debugonly    \nname: "prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__decode__variantvlp__2646a842-c52e-4989-9ec1-ce4048390eeb"\nversion {\n  value: 1\n}\nsignature_name: "serving_stream"\n; RPC from prefill servable to decode servable failed; Failed to close the streaming context; status = UNAVAILABLE: Preempted out of decode queue by a higher priority request.;  Failed to run inference for model: go/debugonly    \nname: "prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__decode__variantvlp__2646a842-c52e-4989-9ec1-ce4048390eeb"\nversion {\n  value: 1\n}\nsignature_name: "serving_stream"\n; RPC from prefill servable to decode servable failed\n=== Source Location Trace: === \nnet/rpc/common/stream/stream-context.cc:1469\nlearning/serving/servables/wiz/remote_wiz_servable.cc:192\nlearning/serving/servables/wiz/prefill_remote_wiz_servable.cc:229\nlearning/serving/servables/wiz/wiz_servable.cc:2600\n;  Failed to run inference for model: go/debugstr    \nname: "prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__prefill__variantvlp__001193af-c90d-41e0-b292-4538df6c40dc"\nversion {\n  value: 1\n}\nsignature_name: "serving_stream"\n); [model=/aistudio/gemini-v3p1s-rev19-calmriver-sc,target=anonymous_server];  for endpoint: /aistudio/gemini-v3p1s-rev19-calmriver-sc [google.rpc.error_details_ext] { message: "The model is overloaded. Please try again later." details { type_url: "type.googleapis.com/language_labs.genai.debug.GeminiApiDebugInfo" value: "\\212\\001\\316\\016\\n\\233\\016Fail to execute model for flow_id: gemini_v3p1s_rev19_calmriver\\nError: A retriable error could not be retried because a stream payload was received from the server (see go/xs-unable-to-retry-message-received) (old status: generic::unavailable: Preempted out of decode queue by a higher priority request.;  Failed to run inference for model: go/debugonly    \\nname: \\"prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__decode__variantvlp__2646a842-c52e-4989-9ec1-ce4048390eeb\\"\\nversion {\\n  value: 1\\n}\\nsignature_name: \\"serving_stream\\"\\n; RPC from prefill servable to decode servable failed; Failed to close the streaming context; status = UNAVAILABLE: Preempted out of decode queue by a higher priority request.;  Failed to run inference for model: go/debugonly    \\nname: \\"prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__decode__variantvlp__2646a842-c52e-4989-9ec1-ce4048390eeb\\"\\nversion {\\n  value: 1\\n}\\nsignature_name: \\"serving_stream\\"\\n; RPC from prefill servable to decode servable failed\\n=== Source Location Trace: === \\nnet/rpc/common/stream/stream-context.cc:1469\\nlearning/serving/servables/wiz/remote_wiz_servable.cc:192\\nlearning/serving/servables/wiz/prefill_remote_wiz_servable.cc:229\\nlearning/serving/servables/wiz/wiz_servable.cc:2600\\n;  Failed to run inference for model: go/debugstr    \\nname: \\"prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__prefill__variantvlp__001193af-c90d-41e0-b292-4538df6c40dc\\"\\nversion {\\n  value: 1\\n}\\nsignature_name: \\"serving_stream\\"\\n); [model=/aistudio/gemini-v3p1s-rev19-calmriver-sc,target=anonymous_server];  for endpoint: /aistudio/gemini-v3p1s-rev19-calmriver-sc\\022.net/rpc/common/stream/stream-context.cc:1470:0" } } [production.rpc.stubs.proto.ExtensibleStubsBackendErrors] { errors { code: 14 space: "generic" message: "Preempted out of decode queue by a higher priority request.;  Failed to run inference for model: go/debugonly    \\nname: \\"prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__decode__variantvlp__2646a842-c52e-4989-9ec1-ce4048390eeb\\"\\nversion {\\n  value: 1\\n}\\nsignature_name: \\"serving_stream\\"\\n; RPC from prefill servable to decode servable failed; Failed to close the streaming context; status = UNAVAILABLE: Preempted out of decode queue by a higher priority request.;  Failed to run inference for model: go/debugonly    \\nname: \\"prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__decode__variantvlp__2646a842-c52e-4989-9ec1-ce4048390eeb\\"\\nversion {\\n  value: 1\\n}\\nsignature_name: \\"serving_stream\\"\\n; RPC from prefill servable to decode servable failed\\n=== Source Location Trace: === \\nnet/rpc/common/stream/stream-context.cc:1469\\nlearning/serving/servables/wiz/remote_wiz_servable.cc:192\\nlearning/serving/servables/wiz/prefill_remote_wiz_servable.cc:229\\nlearning/serving/servables/wiz/wiz_servable.cc:2600\\n;  Failed to run inference for model: go/debugstr    \\nname: \\"prod-common-global__/aistudio/gemini-v3p1s-rev19-calmriver-sc__main__/aistudio/gemini-v3p1s-rev19-calmriver-sc__2025103001__prefill__variantvlp__001193af-c90d-41e0-b292-4538df6c40dc\\"\\nversion {\\n  value: 1\\n}\\nsignature_name: \\"serving_stream\\"\\n" } } 525002922 { 3 { 1: "/aistudio/gemini-v3p1s-rev19-calmriver-sc" } 6 { 4 { 1 { 13: 0x6c65646f } 3 { 2 { 1 { 1: "long-context" 2: 1048576 3: 65536 } 1 { 1 { 13: 0x6c65646f } 2: 131072 3: 65536 } 2 { 1 { 13: 0x6c65646f } 2: 114517 3: 1 } } } } 5 { 1 { 1: "/aistudio/gemini-v3p1s-rev19-calmriver-sc" 3: "serving_stream" } 2: 131072 3: 1 } } }'}]}}
